{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pawel123r\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "while 'models' not in os.listdir():\n",
    "    os.chdir('..')\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "for bucket in s3.buckets.all():\n",
    "    print(bucket.name)\n",
    "\n",
    "BUCKET_NAME = 'pawel123r'\n",
    "train_set = 'data/train.csv'\n",
    "val_set = 'data/val.csv'\n",
    "\n",
    "def download_file(file_name: str, bucket: str):\n",
    "    s3_client.download_file(bucket, file_name, f\"temp/{file_name.split('/')[-1]}\")  \n",
    "\n",
    "def download_return_dataframe(file_name: str, bucket: str):\n",
    "    download_file(file_name, bucket)\n",
    "    return pd.read_csv(f\"temp/{file_name.split('/')[-1]}\")\n",
    "\n",
    "train = download_return_dataframe(train_set, BUCKET_NAME)\n",
    "val = download_return_dataframe(val_set, BUCKET_NAME)\n",
    "\n",
    "def split(dataframe):\n",
    "    return dataframe['input'], dataframe['label']\n",
    "\n",
    "X_train, y_train = split(train)\n",
    "X_val, y_val = split(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                              i didnt feel humiliated\n",
       "1    i can go from feeling so hopeless to so damned...\n",
       "2     im grabbing a minute to post i feel greedy wrong\n",
       "3    i am ever feeling nostalgic about the fireplac...\n",
       "4                                 i am feeling grouchy\n",
       "Name: input, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    sadness\n",
       "1    sadness\n",
       "2      anger\n",
       "3       love\n",
       "4      anger\n",
       "Name: label, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    im feeling quite sad and sorry for myself but ...\n",
       "1    i feel like i am still looking at a blank canv...\n",
       "2                       i feel like a faithful servant\n",
       "3                    i am just feeling cranky and blue\n",
       "4    i can have for a treat or if i am feeling festive\n",
       "Name: input, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    sadness\n",
       "1    sadness\n",
       "2       love\n",
       "3      anger\n",
       "4        joy\n",
       "Name: label, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_train_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 4 0 3 0] [4 4 3 0 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "enc = LabelEncoder().fit(y_train)\n",
    "y_train_num = enc.transform(y_train)\n",
    "y_val_num = enc.transform(y_val)\n",
    "\n",
    "print(y_train_num[0:5], y_val_num[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_series(df):\n",
    "\n",
    "    df_tokenized = df.apply(lambda setence: wordpunct_tokenize(setence))\n",
    "\n",
    "    # Lemmatize words\n",
    "    def lemmatize_list(words):\n",
    "\n",
    "        lemmatized_words = []\n",
    "\n",
    "        for word in words:\n",
    "            lemmatized_words.append(lemmatizer.lemmatize(word).lower())\n",
    "\n",
    "        return lemmatized_words\n",
    "\n",
    "\n",
    "    df_lemmatized = df_tokenized.apply(lambda words: lemmatize_list(words))\n",
    "        \n",
    "    return df_lemmatized\n",
    "\n",
    "X_train_lem = lemmatize_series(X_train)\n",
    "X_val_lem = lemmatize_series(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/glove.6B.zip: 862MB [02:42, 5.32MB/s]                               \n",
      "100%|█████████▉| 399999/400000 [00:24<00:00, 16176.05it/s]\n"
     ]
    }
   ],
   "source": [
    "import torchtext as text\n",
    "\n",
    "vec = text.vocab.GloVe(name='6B', dim=300)\n",
    "\n",
    "# Vectorize\n",
    "def vectorize(words):\n",
    "    return vec.get_vecs_by_tokens(words)\n",
    "\n",
    "X_train_vec = X_train_lem.apply(lambda words: vectorize(words))\n",
    "X_val_vec = X_val_lem.apply(lambda words: vectorize(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.functional import one_hot\n",
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "class Emotions(Dataset):\n",
    "\n",
    "    def __init__(self, inputs, labels, num_classes):\n",
    "\n",
    "        self.inputs = pad_sequence(inputs, batch_first=True, padding_value=0)\n",
    "        self.labels = labels\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def __len__(self):\n",
    "        assert len(self.inputs) == len(self.labels), f\"Length of inputs ({len(self.inputs)}) and labels ({len(self.labels)}) lists don't match\"\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        input = self.inputs[index]\n",
    "        label = torch.tensor(self.labels[index])\n",
    "\n",
    "        return input, one_hot(label, self.num_classes)\n",
    "    \n",
    "num_classes = len(np.unique(y_train_num))\n",
    "traindataset = Emotions(X_train_vec, y_train_num, num_classes)\n",
    "valdataset = Emotions(X_val_vec, y_val_num, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.1329,  0.1699, -0.1436,  ..., -0.2378,  0.1477,  0.6290],\n",
       "         [ 0.1305, -0.1191, -0.4308,  ..., -0.2434, -0.2493,  0.5582],\n",
       "         [ 0.1463, -0.0660,  0.0798,  ...,  0.4928, -0.0553, -0.1069],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]),\n",
       " tensor([0, 0, 1, 0, 0, 0]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindataset.__getitem__(69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "trainLoader = DataLoader(traindataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "valLoader = DataLoader(valdataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 6])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchtext\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, batch_size, num_classes, kwargs = None):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        if kwargs is not None:\n",
    "            self.kwargs = kwargs\n",
    "        else:\n",
    "            self.kwargs = {\n",
    "                'input_size': 300,\n",
    "                'hidden_size': 100,\n",
    "                'num_layers': 4,\n",
    "                'batch_first': True,\n",
    "                'dropout': 0.05,\n",
    "            }\n",
    "        \n",
    "        assert self.kwargs['batch_first'], \"batch_first must be true!\"\n",
    "\n",
    "        self.lstm = nn.LSTM(**self.kwargs)\n",
    "        self.linear1 = nn.Linear(self.kwargs['hidden_size'], self.kwargs['hidden_size']//2)\n",
    "        self.linear2 = nn.Linear(self.kwargs['hidden_size']//2, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.h0 = torch.zeros(size=(self.kwargs['num_layers'], batch_size, self.kwargs['hidden_size']))\n",
    "        self.c0 = torch.zeros(size=(self.kwargs['num_layers'], batch_size, self.kwargs['hidden_size']))\n",
    "\n",
    "        self.hc = (self.h0, self.c0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x, _ = self.lstm(x, self.hc)\n",
    "        # print(x[:,-1,:][0])\n",
    "        x = self.relu(self.linear1(x[:,-1,:]))\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "model = LSTM(batch_size=batch_size, num_classes=6)\n",
    "inputs, labels = next(iter(trainLoader))\n",
    "\n",
    "output = model(inputs)\n",
    "\n",
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN No epoch/batch: 1/ 20] loss: 1.722\n",
      "[TRAIN No epoch/batch: 1/ 40] loss: 1.659\n",
      "[TRAIN No epoch/batch: 1/ 60] loss: 1.610\n",
      "[TRAIN No epoch/batch: 1/ 80] loss: 1.610\n",
      "[TRAIN No epoch/batch: 1/100] loss: 1.567\n",
      "[TRAIN No epoch/batch: 1/120] loss: 1.619\n",
      "[TRAIN No epoch/batch: 1/140] loss: 1.567\n",
      "[TRAIN No epoch/batch: 1/160] loss: 1.573\n",
      "[TRAIN No epoch/batch: 1/180] loss: 1.580\n",
      "[TRAIN No epoch/batch: 1/200] loss: 1.546\n",
      "[TRAIN No epoch/batch: 1/220] loss: 1.564\n",
      "[TRAIN No epoch/batch: 1/240] loss: 1.571\n",
      "[No epoch:     1] train loss: 1.598\n",
      "[TEST No batch:  20] loss: 1.581\n",
      "[No epoch:     1] val loss: 1.581\n",
      "[TRAIN No epoch/batch: 2/ 20] loss: 1.554\n",
      "[TRAIN No epoch/batch: 2/ 40] loss: 1.558\n",
      "[TRAIN No epoch/batch: 2/ 60] loss: 1.580\n",
      "[TRAIN No epoch/batch: 2/ 80] loss: 1.584\n",
      "[TRAIN No epoch/batch: 2/100] loss: 1.589\n",
      "[TRAIN No epoch/batch: 2/120] loss: 1.588\n",
      "[TRAIN No epoch/batch: 2/140] loss: 1.578\n",
      "[TRAIN No epoch/batch: 2/160] loss: 1.567\n",
      "[TRAIN No epoch/batch: 2/180] loss: 1.595\n",
      "[TRAIN No epoch/batch: 2/200] loss: 1.573\n",
      "[TRAIN No epoch/batch: 2/220] loss: 1.582\n",
      "[TRAIN No epoch/batch: 2/240] loss: 1.556\n",
      "[No epoch:     2] train loss: 1.576\n",
      "[TEST No batch:  20] loss: 1.559\n",
      "[No epoch:     2] val loss: 1.581\n",
      "[TRAIN No epoch/batch: 3/ 20] loss: 1.596\n",
      "[TRAIN No epoch/batch: 3/ 40] loss: 1.578\n",
      "[TRAIN No epoch/batch: 3/ 60] loss: 1.595\n",
      "[TRAIN No epoch/batch: 3/ 80] loss: 1.550\n",
      "[TRAIN No epoch/batch: 3/100] loss: 1.571\n",
      "[TRAIN No epoch/batch: 3/120] loss: 1.559\n",
      "[TRAIN No epoch/batch: 3/140] loss: 1.584\n",
      "[TRAIN No epoch/batch: 3/160] loss: 1.582\n",
      "[TRAIN No epoch/batch: 3/180] loss: 1.560\n",
      "[TRAIN No epoch/batch: 3/200] loss: 1.604\n",
      "[TRAIN No epoch/batch: 3/220] loss: 1.577\n",
      "[TRAIN No epoch/batch: 3/240] loss: 1.570\n",
      "[No epoch:     3] train loss: 1.576\n",
      "[TEST No batch:  20] loss: 1.580\n",
      "[No epoch:     3] val loss: 1.581\n",
      "[TRAIN No epoch/batch: 4/ 20] loss: 1.576\n",
      "[TRAIN No epoch/batch: 4/ 40] loss: 1.564\n",
      "[TRAIN No epoch/batch: 4/ 60] loss: 1.598\n",
      "[TRAIN No epoch/batch: 4/ 80] loss: 1.557\n",
      "[TRAIN No epoch/batch: 4/100] loss: 1.568\n",
      "[TRAIN No epoch/batch: 4/120] loss: 1.590\n",
      "[TRAIN No epoch/batch: 4/140] loss: 1.598\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 79\u001b[0m\n\u001b[1;32m     76\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mSGD(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m1e-2\u001b[39m, momentum\u001b[39m=\u001b[39m\u001b[39m0.9\u001b[39m) \n\u001b[1;32m     77\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m---> 79\u001b[0m train(model, trainLoader, valLoader, optimizer, criterion, \u001b[39m50\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[36], line 49\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, trainloader, valloader, optimizer, criterion, epochs)\u001b[0m\n\u001b[1;32m     45\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     47\u001b[0m \u001b[39m# print('Sizes: ', X.size(), y.size(), type(X[0][0][0]))\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[39m# print('Samples: ', X[0], y[0])\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m y_pred \u001b[39m=\u001b[39m model(X)\n\u001b[1;32m     50\u001b[0m \u001b[39m# print('Sizes: ', y_pred.size())\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[39m# print('Samples: ', y_pred[0])\u001b[39;00m\n\u001b[1;32m     52\u001b[0m loss \u001b[39m=\u001b[39m criterion(y_pred, y\u001b[39m.\u001b[39mfloat())\n",
      "File \u001b[0;32m/opt/miniconda3/envs/deepsenseai/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[35], line 38\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 38\u001b[0m     x, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(x, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhc)\n\u001b[1;32m     39\u001b[0m     \u001b[39m# print(x[:,-1,:][0])\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear1(x[:,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,:]))\n",
      "File \u001b[0;32m/opt/miniconda3/envs/deepsenseai/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/miniconda3/envs/deepsenseai/lib/python3.10/site-packages/torch/nn/modules/rnn.py:812\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_forward_args(\u001b[39minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    811\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 812\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[1;32m    813\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[1;32m    814\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    815\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[1;32m    816\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def test(model, dataloader, criterion):\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        running_loss = 0\n",
    "        test_loss = 0\n",
    "        k = 20\n",
    "        \n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            \n",
    "            try:\n",
    "                # print('Sizes: ', X.size(), y.size(), type(X[0][0][0]))\n",
    "                # print('Samples: ', X[0], y[0])\n",
    "                y_pred = model(X)\n",
    "                # print('Sizes: ', y_pred.size())\n",
    "                # print('Samples: ', y_pred[0])\n",
    "                \n",
    "                loss = criterion(y_pred, y.float())\n",
    "\n",
    "                test_loss += loss.item()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                if (batch + 1) % k == 0:\n",
    "                    print(f'[TEST No batch: {batch + 1:3d}] loss: {running_loss / k:.3f}')\n",
    "                    running_loss = 0.0\n",
    "            except Exception as exp:\n",
    "                print(exp)\n",
    "                print(X.size(), y.size())\n",
    "        \n",
    "        return {'test_loss': test_loss/len(dataloader)}\n",
    "\n",
    "def train(model, trainloader, valloader, optimizer, criterion, epochs):\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    k = 20\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        total_loss = 0\n",
    "        running_loss = 0\n",
    "\n",
    "        for batch, (X, y) in enumerate(trainloader):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # print('Sizes: ', X.size(), y.size(), type(X[0][0][0]))\n",
    "            # print('Samples: ', X[0], y[0])\n",
    "            y_pred = model(X)\n",
    "            # print('Sizes: ', y_pred.size())\n",
    "            # print('Samples: ', y_pred[0])\n",
    "            loss = criterion(y_pred, y.float())\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if (batch + 1) % k == 0:\n",
    "                print(f'[TRAIN No epoch/batch: {epoch + 1}/{batch + 1:3d}] loss: {running_loss / k:.3f}')\n",
    "                running_loss = 0.0\n",
    "            \n",
    "        #     break\n",
    "        # break\n",
    "        print(f'[No epoch: {epoch + 1:5d}] train loss: {total_loss / len(trainloader):.3f}')\n",
    "        train_losses.append(total_loss / len(trainloader))\n",
    "\n",
    "        val_result = test(model, valloader, criterion)\n",
    "        print(f\"[No epoch: {epoch + 1:5d}] val loss: {val_result['test_loss']:.3f}\")\n",
    "        val_losses.append(val_result['test_loss'])\n",
    "\n",
    "    return {'train_losses': train_losses, 'val_losses': val_losses}\n",
    "\n",
    "model = LSTM(batch_size=batch_size, num_classes=6)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9) \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train(model, trainLoader, valLoader, optimizer, criterion, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepsenseai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
